# Bayesian Optimization - Reference Card

## One-Liner Usage

```python
from src.pipelines.train_pipeline import main
main(use_bayesian_optimization=True, n_trials=50)
```

## Function Signatures

### `bayesian_optimize_models(X, y, n_trials=50, cv_folds=5)`
Performs Bayesian optimization for hyperparameter tuning.

```python
results, best_model_name = bayesian_optimize_models(
    X_train, 
    y_train, 
    n_trials=50,      # Trials: 20 (fast), 50 (standard), 100 (thorough)
    cv_folds=5        # Cross-validation folds
)
```

**Returns:**
- `results`: Dict with optimization results for each model
- `best_model_name`: String name of best performing model

**Access results:**
```python
results['XGBoost']['mean_auc']      # 0.8523
results['XGBoost']['params']        # {'n_estimators': 203, ...}
results['XGBoost']['best_trial']    # 31
```

### `train_final_model(X_train, y_train, X_test, y_test, model_name, optimized_params=None)`
Trains final model with optional optimized hyperparameters.

```python
model, y_prob = train_final_model(
    X_train, y_train,
    X_test, y_test,
    'XGBoost',
    optimized_params=results['XGBoost']['params']  # Optional
)
```

### `main(use_bayesian_optimization=False, n_trials=50)`
Main training pipeline with switchable approach.

```python
# Standard approach (default)
main()

# Bayesian optimization
main(use_bayesian_optimization=True, n_trials=50)
```

## Common Tasks

### Task 1: Quick Test (5 min)
```python
from src.pipelines.train_pipeline import main
main(use_bayesian_optimization=True, n_trials=20)
```

### Task 2: Production Run (30 min)
```python
from src.pipelines.train_pipeline import main
main(use_bayesian_optimization=True, n_trials=50)  # ‚≠ê RECOMMENDED
```

### Task 3: Thorough Optimization (60 min)
```python
from src.pipelines.train_pipeline import main
main(use_bayesian_optimization=True, n_trials=100)
```

### Task 4: Compare Approaches
```python
from src.pipelines.train_pipeline import main

print("=== Standard ===")
main(use_bayesian_optimization=False)

print("\n=== Bayesian ===")
main(use_bayesian_optimization=True, n_trials=50)
```

### Task 5: Access Optimized Parameters
```python
from src.pipelines.train_pipeline import bayesian_optimize_models

results, best_name = bayesian_optimize_models(X_train, y_train)

# Get parameters
params = results[best_name]['params']
print(params)

# Use them
model = YourModel(**params)
```

## Hyperparameter Search Ranges

### Random Forest (5 parameters)
```
n_estimators:     50 - 300
max_depth:        5 - 30
min_samples_split: 2 - 20
min_samples_leaf:  1 - 10
max_features:     [sqrt, log2]
```

### XGBoost (7 parameters)
```
n_estimators:     50 - 300
max_depth:        3 - 15
learning_rate:    0.01 - 0.3 (log)
subsample:        0.5 - 1.0
colsample_bytree: 0.5 - 1.0
reg_alpha:        0.0 - 10.0
reg_lambda:       0.0 - 10.0
```

### Gradient Boosting (6 parameters)
```
n_estimators:      50 - 300
max_depth:         3 - 15
learning_rate:     0.01 - 0.3 (log)
subsample:         0.5 - 1.0
min_samples_split: 2 - 20
min_samples_leaf:  1 - 10
```

## Output Reading Guide

```
============================================================
üîç BAYESIAN OPTIMIZATION - HYPERPARAMETER TUNING
============================================================
   Trials: 50 | CV Folds: 5

üìä Optimizing Random Forest...
[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 50 completed
   ‚úÖ Best Trial #28           ‚Üê Which trial was best
   Best AUC: 0.8412           ‚Üê Best objective value
   CV AUC: 0.8412 (+/- 0.0128) ‚Üê Cross-validation result
   Best Hyperparameters:       ‚Üê Optimized hyperparameters
      ‚Ä¢ n_estimators: 187
      ‚Ä¢ max_depth: 22
      ...

üìà OPTIMIZATION SUMMARY
XGBoost              0.8523     ‚Üê Final sorted results
Random Forest        0.8412
Gradient Boosting    0.8234

üèÜ BEST MODEL: XGBoost        ‚Üê Winner
   AUC: 0.8523 (+/- 0.0224)

üéØ Training final XGBoost model...
   Using optimized hyperparameters from Bayesian optimization
```

## Timing Guide

| Setting | Time | Use Case |
|---------|------|----------|
| n_trials=20 | 5-10 min | Quick test |
| n_trials=50 | 20-40 min | Production ‚≠ê |
| n_trials=100 | 40-80 min | Final optimization |

## Environment Setup

```bash
# Install if needed
pip install optuna

# Verify
python -c "import optuna; print('‚úÖ Optuna installed')"
```

## File Locations

| File | Purpose |
|------|---------|
| `src/pipelines/train_pipeline.py` | Main implementation |
| `entrypoint/train_with_bayesian_optimization.py` | Example script |
| `docs/BAYESIAN_OPTIMIZATION_QUICKSTART.md` | Quick reference |
| `docs/bayesian_optimization.md` | Full documentation |
| `docs/COMPARISON_STANDARD_VS_BAYESIAN.md` | Detailed comparison |
| `docs/IMPLEMENTATION_SUMMARY.md` | Complete overview |

## Performance Impact

```
Default hyperparameters:   AUC = 0.8156
Bayesian optimized (50):   AUC = 0.8412  (+3.14%)
Bayesian optimized (100):  AUC = 0.8456  (+3.68%)
```

## Key Classes/Functions

```python
# Optuna components
optuna.samplers.TPESampler          # Bayesian optimizer
optuna.pruners.MedianPruner         # Early stopping

# Main functions
bayesian_optimize_models()          # New: Bayesian optimization
compare_models()                    # Existing: Standard comparison
train_final_model()                 # Updated: Supports optimized params
main()                              # Updated: Switchable approach

# Model classes (unchanged)
RandomForestClassifier              # scikit-learn
XGBClassifier                       # xgboost
GradientBoostingClassifier          # scikit-learn
```

## Customization Cheat Sheet

### Change number of trials
```python
main(use_bayesian_optimization=True, n_trials=100)  # More thorough
main(use_bayesian_optimization=True, n_trials=20)   # Faster
```

### Change CV folds
```python
from src.pipelines.train_pipeline import bayesian_optimize_models
results, best = bayesian_optimize_models(X, y, n_trials=50, cv_folds=3)  # Faster
```

### Change hyperparameter range
Edit `src/pipelines/train_pipeline.py`, find the `objective` function:
```python
# Before
'n_estimators': trial.suggest_int('n_estimators', 50, 300)

# After (wider range)
'n_estimators': trial.suggest_int('n_estimators', 100, 500)
```

### Change optimization metric
In objective function, change scoring:
```python
# Before
scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)

# After
scores = cross_val_score(model, X, y, cv=cv, scoring='f1', n_jobs=-1)
```

## Troubleshooting

| Issue | Fix |
|-------|-----|
| `ImportError: No module named optuna` | `pip install optuna` |
| Takes too long | Use `n_trials=20` or `cv_folds=3` |
| Different results on re-run | Normal; seeds are fixed but slight variance |
| Out of memory | Reduce data size or use `cv_folds=3` |
| Model not improving | Try `n_trials=100` or adjust hyperparameter ranges |

## Quick Decision Tree

```
Do you want automatic hyperparameter tuning?
‚îú‚îÄ NO ‚Üí Use compare_models() or main(False)
‚îî‚îÄ YES
   ‚îú‚îÄ Have 5-10 min? ‚Üí main(True, n_trials=20)
   ‚îú‚îÄ Have 20-40 min? ‚Üí main(True, n_trials=50) ‚≠ê
   ‚îî‚îÄ Have 40+ min? ‚Üí main(True, n_trials=100)
```

## Export Results

```python
# Print summary
for model_name in results:
    print(f"{model_name}: AUC={results[model_name]['mean_auc']:.4f}")

# Save parameters
import json
best_params = results[best_name]['params']
with open('best_params.json', 'w') as f:
    json.dump(best_params, f, indent=2)

# Load parameters
with open('best_params.json', 'r') as f:
    loaded_params = json.load(f)
```

## Verify Installation

```python
# Check Optuna
import optuna
print(f"Optuna version: {optuna.__version__}")

# Check function exists
from src.pipelines.train_pipeline import bayesian_optimize_models
print("‚úÖ bayesian_optimize_models available")

# Quick smoke test
print("‚úÖ All imports successful")
```

## Typical Workflow

1. **Load data**
   ```python
   X, y, feature_names = load_processed_data()
   X_train, X_test, y_train, y_test = train_test_split(X, y)
   ```

2. **Run optimization**
   ```python
   results, best_name = bayesian_optimize_models(X_train, y_train)
   ```

3. **Train final model**
   ```python
   model, y_prob = train_final_model(
       X_train, y_train, X_test, y_test,
       best_name,
       optimized_params=results[best_name]['params']
   )
   ```

4. **Evaluate**
   ```python
   from sklearn.metrics import roc_auc_score
   auc = roc_auc_score(y_test, y_prob)
   print(f"Test AUC: {auc:.4f}")
   ```

5. **Save**
   ```python
   save_model(model, feature_names)
   ```

## Command-Line Usage

```bash
# Run example script
python entrypoint/train_with_bayesian_optimization.py

# Run with standard method
python entrypoint/train.py

# Run custom script
python -c "from src.pipelines.train_pipeline import main; main(True, 50)"
```

## Best Practices

‚úÖ Use `n_trials=50` for production models
‚úÖ Use `cv_folds=5` for robust evaluation
‚úÖ Keep hyperparameter ranges reasonable
‚úÖ Check progress bar during optimization
‚úÖ Save optimized parameters for reproducibility
‚úÖ Compare with standard approach to measure improvement
‚úÖ Use fixed seeds for reproducibility

‚ùå Don't use extreme hyperparameter ranges
‚ùå Don't reduce CV folds below 3
‚ùå Don't optimize too many hyperparameters at once
‚ùå Don't ignore convergence warnings

---

**Remember**: With `n_trials=50`, expect 20-40 minutes of computation.
For production, that's worth the +2-5% AUC improvement! üöÄ
